{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0elka6xwNCji"
   },
   "outputs": [],
   "source": [
    "############################################################## START ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "My24R5HvAXlI"
   },
   "outputs": [],
   "source": [
    "!wget http://www.statmt.org/europarl/v7/europarl.tgz\n",
    "!tar zxvf europarl.tgz\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoHgUKDtsT4L"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import ast\n",
    "import copy\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "VqjOBfDuEGZM"
   },
   "outputs": [],
   "source": [
    "SEED=44\n",
    "EPOCHS=5\n",
    "DEVICE=\"gpu\" \n",
    "DROPOUT=0.2\n",
    "BATCH_SIZE=16\n",
    "L2_DECAY=0.001\n",
    "LEARNING_RATE=1e-4\n",
    "CHAR_VECTOR_DIM=100\n",
    "# CLIP=False\n",
    "if DEVICE == \"gpu\":\n",
    "    DEVICE=(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jlVVKjG9SD0g",
    "outputId": "3eb4b4ef-0395-43f6-e9b6-3a12ecbc400a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'da': tensor([6]),\n",
       " 'de': tensor([7]),\n",
       " 'en': tensor([0]),\n",
       " 'es': tensor([5]),\n",
       " 'fr': tensor([1]),\n",
       " 'it': tensor([2]),\n",
       " 'nl': tensor([4]),\n",
       " 'pt': tensor([3])}"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ALL CLASSES: ['bg', 'cs', 'da', 'de', 'el', 'en', 'es', 'et', 'fi', 'fr', 'hu', 'it', 'lt', 'lv', 'nl', 'pl', 'pt', 'ro', 'sk', 'sl', 'sv']\n",
    "LABELS = ['en', 'fr', 'it', 'pt', 'nl', 'es', 'da', 'de']\n",
    "LABEL_MAP = {}\n",
    "\n",
    "for i in range(len(LABELS)):\n",
    "    # LABEL_MAP[LABELS[i]] = torch.zeros(len(LABELS))\n",
    "    # LABEL_MAP[LABELS[i]][i] = 1\n",
    "    LABEL_MAP[LABELS[i]] = torch.as_tensor([i]) \n",
    "LABEL_MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pL-NQhhdlZVz"
   },
   "outputs": [],
   "source": [
    "char_set = {'a'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EZ1h5G5iPpbm",
    "outputId": "5caa5c59-edad-4dd0-f478-ce78ba6f6494"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/9672 [00:00<01:32, 104.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9672/9672 [00:01<00:00, 8548.27it/s]\n",
      "  0%|          | 0/9450 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9450/9450 [00:01<00:00, 7645.70it/s]\n",
      "  0%|          | 18/9486 [00:00<01:37, 97.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9486/9486 [00:01<00:00, 7997.97it/s]\n",
      "  0%|          | 18/9434 [00:00<01:39, 94.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9434/9434 [00:01<00:00, 7879.56it/s]\n",
      "  0%|          | 18/9433 [00:00<01:39, 94.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9433/9433 [00:01<00:00, 8275.13it/s]\n",
      "  0%|          | 0/9433 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "es\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9433/9433 [00:01<00:00, 7896.53it/s]\n",
      "  0%|          | 18/9373 [00:00<01:31, 101.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9373/9373 [00:01<00:00, 8169.07it/s]\n",
      "  0%|          | 16/9224 [00:00<01:49, 83.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9224/9224 [00:01<00:00, 7704.28it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_dataset(path='txt', op_path='dataset', limit=10000, filter=[]):\n",
    "    LONGEST_WORD = (0, '')\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    try:\n",
    "        os.mkdir(op_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    ind = 0\n",
    "    for dir in filter:\n",
    "        lang = dir\n",
    "        ctr, k = 0, 0\n",
    "        print(lang)\n",
    "        for txt in tqdm.tqdm(os.listdir(path+'/'+dir), position=0, leave=True):\n",
    "            try:\n",
    "                df = []\n",
    "                ind += 1\n",
    "                for line in open(path+'/'+dir+'/'+txt, \"r\"):\n",
    "                    # ctr += 1\n",
    "                    for word in line.split():\n",
    "                        for punct in ['/', '\\\\', '_']:\n",
    "                            if punct in word:\n",
    "                                line = line.replace(word, \"\")\n",
    "                    line = re.sub(\"<.*>\", \"\", line)\n",
    "                    line = re.sub(r'http\\S+', '', line)\n",
    "                    for punct in ['\\t','\\n','!','\"','#','$','%','&',\"'\",')','(','*','+',',','-','.','/',':',';','<','=','>','?','@','^','//']:\n",
    "                        line = line.replace(punct, \" \")\n",
    "                    for punct in ['.']:\n",
    "                        line = line.replace(punct, \" \")\n",
    "                    line = ' '.join(line.split())\n",
    "\n",
    "                    for letter in line:\n",
    "                        char_set.add(letter)\n",
    "                    if line != '':\n",
    "                        k += 1\n",
    "                        for word in line.split():\n",
    "                            if len(word) > LONGEST_WORD[0]:\n",
    "                                LONGEST_WORD = (len(word), word)\n",
    "                        inputs.append(line)\n",
    "                        targets.append(lang)\n",
    "                        if k == limit:\n",
    "                            break\n",
    "                    if k == limit:\n",
    "                        break\n",
    "                # df.to_csv(op_path+f\"/{ind}.csv\")\n",
    "            except UnicodeDecodeError:\n",
    "                print(path+'/'+dir+'/'+txt)\n",
    "        # data_dist[lang] = ctr\n",
    "    return LONGEST_WORD, inputs, targets\n",
    "filter = ['en', 'fr', 'it', 'pt', 'nl', 'es', 'da', 'de']\n",
    "LONGEST_WORD, inputs, targets = process_dataset(filter=filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1NP2nziun6qE"
   },
   "outputs": [],
   "source": [
    "# data_dist = {}\n",
    "# for lang in LABEL_MAP:\n",
    "#     data_dist[lang]=0  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7UnnIeOovLH8"
   },
   "outputs": [],
   "source": [
    "# for k,v in sorted(data_dist.items(), key=lambda x: x[1], reverse=True):\n",
    "#     print(k, v)\n",
    "\n",
    "### Sentences per language\n",
    "\"\"\"\n",
    "en 1663354\n",
    "fr 1637529\n",
    "it 1626908\n",
    "pt 1616234\n",
    "nl 1615893\n",
    "es 1611610\n",
    "da 1603101\n",
    "de 1567229\n",
    "sv 1562275\n",
    "fi 1536972\n",
    "el 1193948\n",
    "et 476752\n",
    "cs 476020\n",
    "sk 470712\n",
    "lt 470624\n",
    "lv 469128\n",
    "pl 467987\n",
    "hu 461872\n",
    "sl 458136\n",
    "bg 287826\n",
    "ro 283680\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q6X6VFX7ofXT",
    "outputId": "d3edf428-adab-46c1-a92a-1a644445d7cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_list = sorted(list(char_set))\n",
    "char_map = {}\n",
    "for i, char in enumerate(char_list):\n",
    "    char_map[char]=i\n",
    "len(char_map)\n",
    "# char_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gkb0CkWdOCWu",
    "outputId": "5081e72d-1217-4d35-f7b4-b0b0613b89f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 'infrastrukturmoderniseringsforanstaltningerne')"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LONGEST_WORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ubql8Yll9AEU"
   },
   "outputs": [],
   "source": [
    "class CharTokenizer():\n",
    "    def __init__(self, char_map, label_map=LABEL_MAP):\n",
    "        self.map = char_map\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def encode(self, sentence, pad=47):\n",
    "        res = []\n",
    "        for word in sentence.split():\n",
    "            temp = []\n",
    "            for letter in word:\n",
    "                temp.append(self.map[letter])\n",
    "            while len(temp)<pad:\n",
    "                temp.append(0)\n",
    "            temp = torch.as_tensor(temp)\n",
    "            res.append(temp)\n",
    "        res = torch.stack(res)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def encode_target(self, label):\n",
    "        return self.label_map[label]\n",
    "\n",
    "    def encode_dataset(self, sentences, targets):\n",
    "        res = []\n",
    "        tgts = []\n",
    "        for i in range(len(sentences)):\n",
    "            res.append(self.encode(sentences[i]))\n",
    "            tgts.append(self.encode_target(targets[i]))\n",
    "        # res = torch.stack(res)\n",
    "        # targets = torch.stack(targets) \n",
    "        tgts = torch.cat(tgts)\n",
    "        return res, tgts\n",
    "tokenizer = CharTokenizer(char_map=char_map, label_map=LABEL_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ndFAIMeDdEZ6"
   },
   "outputs": [],
   "source": [
    "# !find \"batches\" -type f | wc -l\n",
    "# !rm -rf -d \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-tb-tIxNwzUq"
   },
   "outputs": [],
   "source": [
    "# def tokenize_dataset(ip_path='dataset', op_path='processed', batch_size=BATCH_SIZE):\n",
    "#     try:\n",
    "#         os.mkdir(op_path)\n",
    "#     except FileExistsError:\n",
    "#         pass\n",
    "    \n",
    "#     proc, target = [], []\n",
    "#     for item in os.listdir(ip_path):\n",
    "#         df = pd.read_csv(ip_path+'/'+item)\n",
    "#         proc += list(df['text'])\n",
    "#         for label in list(df['label']):\n",
    "#             target.append(LABEL_MAP[label])\n",
    "#         # for item in df.to_dict('record'):\n",
    "#         #      = tokenizer.encode(item['text'])\n",
    "#         #     LABEL_MAP[item['label']]\n",
    "#     proc = tokenizer(proc)\n",
    "#     target = torch.stack(target)\n",
    "\n",
    "#     return proc, target\n",
    "\n",
    "# proc, target = tokenize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YIx1Jyxzs54l"
   },
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, char_set_size=261, hidden_size=50, word_pad=47, num_classes=8):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=char_set_size, embedding_dim=CHAR_VECTOR_DIM)\n",
    "        self.lstm = nn.LSTM(input_size=CHAR_VECTOR_DIM, hidden_size=hidden_size)\n",
    "        self.dropout = nn.Dropout(p=DROPOUT)\n",
    "        self.max_pooler = nn.MaxPool1d(kernel_size=word_pad)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, sentences):\n",
    "        y = []\n",
    "        # print(f\"sentences are: {sentences}\")\n",
    "        for x in sentences:\n",
    "            x.to(DEVICE)\n",
    "            x = self.embedding(x)\n",
    "            x,_ = self.lstm(x)\n",
    "            x = torch.transpose(x[-1:], 1, 2)\n",
    "            x = self.max_pooler(x)\n",
    "            y.append(x[0].squeeze(1))\n",
    "\n",
    "        y = torch.stack(y)\n",
    "        y = self.fc(y)\n",
    "        y = self.softmax(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yq_nmkkIijI",
    "outputId": "e96b50dc-91ef-4869-b953-a4434c1b6684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharLSTM(\n",
      "  (embedding): Embedding(261, 100)\n",
      "  (lstm): LSTM(100, 50)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (max_pooler): MaxPool1d(kernel_size=47, stride=47, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=50, out_features=8, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CharLSTM()\n",
    "model.to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "I3o1GKXY5O-Y"
   },
   "outputs": [],
   "source": [
    "# target_inputs, val_inputs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "x_train, x_test_val, y_train, y_test_val = train_test_split(inputs, targets, test_size=0.2, random_state=42)\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test_val, y_test_val, test_size=0.5, random_state=42)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nm99uF7FEPvC",
    "outputId": "33136496-7dae-46ea-cff9-e533dcd9230c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4000 [00:00<08:10,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [04:45<00:00, 14.02it/s]\n",
      "100%|██████████| 500/500 [00:19<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 1, 'Training Loss': 1.80635404586792, 'Validation Loss': 1.5256998538970947}\n",
      "\n",
      "\n",
      "======== Epoch 2 / 5 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [04:45<00:00, 13.99it/s]\n",
      "100%|██████████| 500/500 [00:19<00:00, 25.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 2, 'Training Loss': 1.4861446619033813, 'Validation Loss': 1.4349178075790405}\n",
      "\n",
      "\n",
      "======== Epoch 3 / 5 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [04:44<00:00, 14.07it/s]\n",
      "100%|██████████| 500/500 [00:18<00:00, 26.57it/s]\n",
      "  0%|          | 2/4000 [00:00<05:26, 12.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 3, 'Training Loss': 1.380031943321228, 'Validation Loss': 1.3585914373397827}\n",
      "\n",
      "\n",
      "======== Epoch 4 / 5 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [04:35<00:00, 14.50it/s]\n",
      "100%|██████████| 500/500 [00:18<00:00, 27.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 4, 'Training Loss': 1.3461110591888428, 'Validation Loss': 1.339268684387207}\n",
      "\n",
      "\n",
      "======== Epoch 5 / 5 ========\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [04:32<00:00, 14.69it/s]\n",
      "100%|██████████| 500/500 [00:18<00:00, 27.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Epoch': 5, 'Training Loss': 1.331242561340332, 'Validation Loss': 1.3295758962631226}\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print('\\n======== Epoch {:} / {:} ========\\n'.format(epoch+1, EPOCHS))\n",
    "    model.train()\n",
    "    train_batch_losses = []\n",
    "    ITERS = int(len(x_train)/BATCH_SIZE)\n",
    "    for i in tqdm.tqdm(range(ITERS), position=0, leave=True):\n",
    "        end = BATCH_SIZE*(i+1)\n",
    "        start = BATCH_SIZE*i\n",
    "        model.zero_grad()\n",
    "        input, target = tokenizer.encode_dataset(x_train[start : end], y_train[start : end])\n",
    "        \n",
    "        for i in range(len(input)):\n",
    "            input[i] = input[i].to(DEVICE)\n",
    "\n",
    "        output = model(input)\n",
    "        output = output.to(DEVICE)\n",
    "        target = target.to(DEVICE)\n",
    "        loss = criterion(output, target)\n",
    "        # print(loss.item())\n",
    "        # data.append({\"output\":output.tolist(), \"target\":target.tolist()})\n",
    "        loss.backward() \n",
    "        optim.step()\n",
    "        train_batch_losses.append(loss)\n",
    "\n",
    "    val_batch_losses = []\n",
    "    model.eval()\n",
    "    ITERS = int(len(x_val)/BATCH_SIZE)\n",
    "    for i in tqdm.tqdm(range(ITERS), position=0, leave=True):\n",
    "        end = BATCH_SIZE*(i+1)\n",
    "        start = BATCH_SIZE*i\n",
    "        input, target = tokenizer.encode_dataset(x_val[start : end], y_val[start : end])\n",
    "\n",
    "        for i in range(len(input)):\n",
    "            input[i] = input[i].to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "            output = output.to(DEVICE)\n",
    "            target = target.to(DEVICE)\n",
    "            loss = criterion(output, target)\n",
    "        # print(loss.item())\n",
    "        # data.append({\"output\":output.tolist(), \"target\":target.tolist()})\n",
    "            val_batch_losses.append(loss)\n",
    "    # for step, batch in enumerate(tqdm.tqdm(val_dataloader)):\n",
    "    #     with torch.no_grad():\n",
    "    #         output = model(input_ids, attn_masks, token_ids)\n",
    "    #         output.to(DEVICE)\n",
    "    #         loss = criterion(output, target)\n",
    "    #             # print(loss.item())\n",
    "    #         val_batch_losses.append(loss)\n",
    "                # val_stats['mse_loss'].append(get_mse(output, target))\n",
    "                # val_stats['r'].append(get_r(output, target))\n",
    "                # val_stats['kl_div_loss'].append(get_kl(output, target))\n",
    "    \n",
    "    train_dict = {\n",
    "                'Epoch': epoch + 1,\n",
    "                'Training Loss': torch.tensor(train_batch_losses).mean().item(),\n",
    "                'Validation Loss': torch.tensor(val_batch_losses).mean().item(),\n",
    "                }\n",
    "   \n",
    "    # curr_val_loss = train_dict['Validation Loss']\n",
    "    # curr_train_loss = train_dict['Training Loss']\n",
    "    # if curr_val_loss < min_val_loss:\n",
    "    #     min_val_loss = curr_val_loss\n",
    "    #     print(\"Saving model ...\")\n",
    "    #     torch.save(model, \"best_model\")\n",
    "\n",
    "    # if curr_train_loss < min_train_loss:\n",
    "    #     min_train_loss = curr_train_loss\n",
    "        \n",
    "    #     if curr_val_loss == min_val_loss:\n",
    "    #         print(\"Saving model ...\")\n",
    "    #         torch.save(model, \"best_model\")\n",
    "    \n",
    "    print(str(train_dict)+'\\n')\n",
    "    training_stats.append(train_dict)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iUSq_s_mpK4W",
    "outputId": "b4734d54-2962-4ce0-9b5f-a583e06c4e8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:22<00:00, 360.17it/s]\n"
     ]
    }
   ],
   "source": [
    "def predict(model, inputs, tokenizer):\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    outputs = []\n",
    "    \n",
    "    invert_map = {}\n",
    "    map = tokenizer.label_map\n",
    "    for key in map:\n",
    "        value = map[key].tolist()[0]\n",
    "        invert_map[value] = key\n",
    "\n",
    "    for input in tqdm.tqdm(inputs, position=0, leave=True):\n",
    "        input = tokenizer.encode(input)\n",
    "        with torch.no_grad():\n",
    "            input = input.to(DEVICE)\n",
    "            output = model([input])\n",
    "            output = output.to('cpu')\n",
    "            outputs.append(invert_map[int(np.argmax(output))])\n",
    "        # print(loss.item())\n",
    "        # data.append({\"output\":output.tolist(), \"target\":target.tolist()})\n",
    "    return outputs\n",
    "\n",
    "preds = predict(model, x_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kq1iRY543U70",
    "outputId": "3c526bd9-bf49-45ee-c700-6c6a12e9d8c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test set class distribution {'Italian': 981, 'English': 965, 'German': 1048, 'Dutch': 963, 'Portugese': 1022, 'French': 974, 'Spanish': 1020, 'Danish': 1027}\n",
      "accuracy = 95.525%\n",
      "class wise accuracies are:\n",
      "Italian: accuracy = 93.68213228035538%\n",
      "English: accuracy = 97.89251844046365%\n",
      "German: accuracy = 87.82755705832629%\n",
      "Dutch: accuracy = 98.59002169197397%\n",
      "Portugese: accuracy = 96.31474103585657%\n",
      "French: accuracy = 97.53483386923901%\n",
      "Spanish: accuracy = 96.91040164778579%\n",
      "Danish: accuracy = 97.36585365853658%\n",
      "f1-micro is 95.525%\n",
      "f1-macro is 95.56758391674758%\n",
      "f1-weighted is 95.50344766088578%\n"
     ]
    }
   ],
   "source": [
    "def get_accuracy(preds, true, norm=True):\n",
    "    score=0\n",
    "    for x,y in zip(preds, true):\n",
    "        if x == y:\n",
    "            score += 1\n",
    "    if norm:\n",
    "        score /= len(true)\n",
    "    return score\n",
    "\n",
    "iso_map = {'nl':'Dutch', 'es':'Spanish', 'it':'Italian', 'fr':'French', 'pt':'Portugese', 'en':'English', 'de':'German', 'da':'Danish'}\n",
    "test_dist = {iso_map[class_]:y_test.count(class_) for class_ in set(y_test)}\n",
    "\n",
    "def get_class_wise_accuracy(preds, true, norm=True):\n",
    "    classes = set(true+preds)\n",
    "    accuracy = {class_:0 for class_ in classes}\n",
    "    totals = {class_:0 for class_ in classes}\n",
    "    \n",
    "    for x,y in zip(preds, true):\n",
    "        if x == y:\n",
    "            accuracy[x] += 1\n",
    "        totals[x] += 1\n",
    "    if norm:\n",
    "        for class_ in accuracy:\n",
    "            accuracy[class_] /= totals[class_] \n",
    "    accuracy = {iso_map[class_]:accuracy[class_] for class_ in accuracy}\n",
    "    return accuracy\n",
    "\n",
    "# print(f\"{y_test}\")\n",
    "\n",
    "print(f\"test set class distribution {test_dist}\")\n",
    "print(f\"accuracy = {get_accuracy(preds, y_test)*100}%\")\n",
    "print(\"class wise accuracies are:\")\n",
    "accuracy = get_class_wise_accuracy(preds, y_test)\n",
    "for class_ in accuracy:\n",
    "    print(f\"{class_}: accuracy = {accuracy[class_]*100}%\")\n",
    "\n",
    "print(f\"f1-micro is {f1_score(preds, y_test, average='micro')*100}%\")\n",
    "print(f\"f1-macro is {f1_score(preds, y_test, average='macro')*100}%\")\n",
    "print(f\"f1-weighted is {f1_score(preds, y_test, average='weighted')*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 418
    },
    "id": "t9jp8tQxIXoN",
    "outputId": "5230181e-bc3b-44f1-a878-a08442d0ba4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "misclassifications:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>true label</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sobre el apartado 5</td>\n",
       "      <td>es</td>\n",
       "      <td>pt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Turno de preguntas preguntas al Consejo</td>\n",
       "      <td>es</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Murder of Rosemary Nelson</td>\n",
       "      <td>en</td>\n",
       "      <td>da</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excluir a determinados usuarios de este sistem...</td>\n",
       "      <td>es</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Muchas gracias señor Söderman</td>\n",
       "      <td>es</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Madame la Commissaire faites preuve de plus de...</td>\n",
       "      <td>fr</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>La ringrazio Commissario Van den Broek</td>\n",
       "      <td>it</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>A Uachtaráin a Leas Uachtaráin ní mór don Choi...</td>\n",
       "      <td>de</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>SPEAKER ID 212 NAME Nils Lundgren</td>\n",
       "      <td>it</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>Kirgisistan</td>\n",
       "      <td>da</td>\n",
       "      <td>de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>358 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text true label prediction\n",
       "0                                  Sobre el apartado 5         es         pt\n",
       "1              Turno de preguntas preguntas al Consejo         es         it\n",
       "2                            Murder of Rosemary Nelson         en         da\n",
       "3    Excluir a determinados usuarios de este sistem...         es         fr\n",
       "4                        Muchas gracias señor Söderman         es         de\n",
       "..                                                 ...        ...        ...\n",
       "353  Madame la Commissaire faites preuve de plus de...         fr         it\n",
       "354             La ringrazio Commissario Van den Broek         it         es\n",
       "355  A Uachtaráin a Leas Uachtaráin ní mór don Choi...         de         en\n",
       "356                  SPEAKER ID 212 NAME Nils Lundgren         it         de\n",
       "357                                        Kirgisistan         da         de\n",
       "\n",
       "[358 rows x 3 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "mismatches = []\n",
    "correct_lengths = []\n",
    "mistake_lengths = []\n",
    "\n",
    "for sent, true, pred in zip(x_test, y_test, preds):\n",
    "    if true != pred:\n",
    "        mismatches.append({'text':sent, 'true label':true, 'prediction':pred})\n",
    "        mistake_lengths.append(len(sent.strip().split()))\n",
    "    else:\n",
    "        correct_lengths.append(len(sent.strip().split()))\n",
    "    lengths.append(len(sent.strip().split()))\n",
    "mismatches = pd.DataFrame(mismatches)\n",
    "print(\"misclassifications:\")\n",
    "mismatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T-b0ri-MW7Ws",
    "outputId": "efb68496-611a-4c6f-bb02-62a443312d2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of length of sentences = 70.706375\n",
      "the median of length of sentences = 63.0\n",
      "the standard deviation of length of sentences = 47.60522985302534\n"
     ]
    }
   ],
   "source": [
    "# analysis of sentence lengths\n",
    "mean = np.mean(lengths)\n",
    "median = np.median(lengths)\n",
    "std_dev = (np.var(lengths))**0.5\n",
    "print(f\"the mean of length of sentences = {mean}\")\n",
    "print(f\"the median of length of sentences = {median}\")\n",
    "print(f\"the standard deviation of length of sentences = {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVvNCgdCKjLN",
    "outputId": "6700ad1d-c649-4f28-863d-d11538d6608a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of length of misclassified sentences = 10.986033519553073\n",
      "the median of length of misclassified sentences = 6.0\n",
      "the standard deviation of length of misclassified sentences = 15.801043930543507\n"
     ]
    }
   ],
   "source": [
    "# analysis of sentence lengths for wrong predictions\n",
    "mean = np.mean(mistake_lengths)\n",
    "median = np.median(mistake_lengths)\n",
    "std_dev = (np.var(mistake_lengths))**0.5\n",
    "print(f\"the mean of length of misclassified sentences = {mean}\")\n",
    "print(f\"the median of length of misclassified sentences = {median}\")\n",
    "print(f\"the standard deviation of length of misclassified sentences = {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4HG2wVEiOXhN",
    "outputId": "46a23712-089b-492d-8845-d81dff573f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the mean of length of correcty classified sentences = 73.50405652970427\n",
      "the median of length of correctly classified sentences = 65.0\n",
      "the standard deviation of length of correctly classified sentences = 46.75277622867615\n"
     ]
    }
   ],
   "source": [
    "# analysis of sentence lengths for correct predictions\n",
    "mean = np.mean(correct_lengths)\n",
    "median = np.median(correct_lengths)\n",
    "std_dev = (np.var(correct_lengths))**0.5\n",
    "print(f\"the mean of length of correcty classified sentences = {mean}\")\n",
    "print(f\"the median of length of correctly classified sentences = {median}\")\n",
    "print(f\"the standard deviation of length of correctly classified sentences = {std_dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qjU8-yV-VT0d"
   },
   "outputs": [],
   "source": [
    "### These are details for model performance on first 10,000 sentences of each language\n",
    "### train : validation : test = 80% : 10% : 10%  \n",
    "\"\"\"\n",
    "test set class distribution {'Spanish': 1020, 'Italian': 981, 'French': 974, 'Portugese': 1022, 'Dutch': 963, 'English': 965, 'German': 1048, 'Danish': 1027}\n",
    "accuracy = 95.875%\n",
    "class wise accuracies are:\n",
    "Spanish: accuracy = 96.07250755287009%\n",
    "Italian: accuracy = 93.78698224852072%\n",
    "French: accuracy = 97.00103412616339%\n",
    "Portugese: accuracy = 96.79037111334003%\n",
    "Dutch: accuracy = 98.69989165763813%\n",
    "English: accuracy = 97.97872340425532%\n",
    "German: accuracy = 90.04366812227073%\n",
    "Danish: accuracy = 97.84524975514202%\n",
    "f1-micro is 95.875%\n",
    "f1-macro is 95.90623350666452%\n",
    "f1-weighted is 95.86089203953024%\n",
    "\"\"\"\n",
    "### Results of sentence length analysis\n",
    "\"\"\"\n",
    "the mean of length of sentences = 70.706375\n",
    "the median of length of sentences = 63.0\n",
    "the standard deviation of length of sentences = 47.60522985302534\n",
    "\n",
    "the mean of length of misclassified sentences = 9.912121212121212\n",
    "the median of length of misclassified sentences = 6.0\n",
    "the standard deviation of length of misclassified sentences = 15.681918898812862\n",
    "\n",
    "the mean of length of correcty classified sentences = 73.32203389830508\n",
    "the median of length of correctly classified sentences = 65.0\n",
    "the standard deviation of length of correctly classified sentences = 46.76883228784532\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scz28ngVrcUY"
   },
   "outputs": [],
   "source": [
    "### These are details for model performance on first 10,000 sentences of each language\n",
    "### train : validation : test = 80% : 10% : 10%  \n",
    "\"\"\"\n",
    "test set class distribution {'Spanish': 12575, 'German': 12523, 'Italian': 12434, 'French': 12722, 'Portugese': 12500, 'English': 12280, 'Dutch': 12474, 'Danish': 12492}\n",
    "accuracy = 98.28%\n",
    "class wise accuracies are:\n",
    "Spanish: accuracy = 98.79644588045234%\n",
    "German: accuracy = 98.16681215776526%\n",
    "Italian: accuracy = 97.92299089311392%\n",
    "French: accuracy = 98.67685279987398%\n",
    "Portugese: accuracy = 96.94271531006224%\n",
    "English: accuracy = 97.97659678205754%\n",
    "Dutch: accuracy = 98.36237495966441%\n",
    "Danish: accuracy = 99.4198694706309%\n",
    "f1-micro is 98.28%\n",
    "f1-macro is 98.27959967758044%\n",
    "f1-weighted is 98.2794312965243%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EugY40MNM5m6"
   },
   "outputs": [],
   "source": [
    "############################################################## END ##############################################################"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "langid.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
